{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/phishing_email.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [re.sub(r'[^a-zA-Z]', '', word) for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in punctuation and word]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "processed_df = copy.deepcopy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df[\"text_combined\"] = processed_df[\"text_combined\"].fillna('').apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for line in processed_df[\"text_combined\"]:\n",
    "    tokens.append(line.strip().split())\n",
    "\n",
    "flat_tokens = [x for xs in tokens for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "vocab = build_vocab_from_iterator(flat_tokens, specials=[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = processed_df['text_combined']\n",
    "y = processed_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text_combined', TfidfVectorizer(stop_words='english', max_features=5000), 'text_combined'),  # TF-IDF for text\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', column_transformer),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "torch.set_default_device(device)\n",
    "print(f\"Using device = {torch.get_default_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy', verbose=10)\n",
    "# print(\"Cross-Validation scores:\", cv_scores)\n",
    "# print(\"Average Cross-Validation:\", np.mean(cv_scores))\n",
    "\n",
    "# param_grid = {\n",
    "#     'classifier__C': [0.1, 1, 10],\n",
    "#     'classifier__solver': ['lbfgs', 'liblinear']\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=4, verbose=10)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# print(\"Best Parameters from GridSearchCV:\", grid_search.best_params_)\n",
    "\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# y_pred = best_model.predict(X_test)\n",
    "\n",
    "# print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# accuracy = np.mean(y_pred == y_test)\n",
    "# print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# joblib.dump(best_model, 'filename.pkl', compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNNet, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(input_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, line_tensor):\n",
    "        rnn_out, hidden = self.rnn(line_tensor)\n",
    "        output = self.h2o(hidden[0])\n",
    "        output = self.softmax(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig:\n",
    "    n_hidden = 128\n",
    "    train_batch_size = 64\n",
    "    eval_batch_size = 64  # how many images to sample during evaluation\n",
    "    num_epochs = 3\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "\n",
    "    seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import unicodedata\n",
    "\n",
    "allowed_characters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(allowed_characters)\n",
    "n_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return allowed_characters.find(letter)\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_df.to_csv(\"processed_data/processed_phish.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "# X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# #Vectorize test texts.\n",
    "# X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform([X_train[0]])\n",
    "X_train = X_train.todense()\n",
    "X_train = torch.tensor(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PhishDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_dir, vocab_size, vocab):\n",
    "        self.data_dir = data_dir #for provenance of the dataset\n",
    "        self.load_time = time.localtime #for provenance of the dataset\n",
    "        labels_set = set() #set of all classes\n",
    "        self.count = 0\n",
    "        self.num_workers = 4\n",
    "        self.data = []\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = vocab_size\n",
    "        #self.data_tensors = []\n",
    "        self.labels = []\n",
    "        #self.labels_tensors = []\n",
    "\n",
    "        #read all the ``.csv`` files in the specified directory\n",
    "        text_files = glob.glob(os.path.join(data_dir, '*.csv'))\n",
    "        for filename in text_files:\n",
    "            label = os.path.splitext(os.path.basename(filename))[0]\n",
    "            labels_set.add(label)\n",
    "            lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "            for line in lines:\n",
    "                self.data.append(line)\n",
    "                #self.data_tensors.append(lineToTensor(line))\n",
    "                self.labels.append(label)\n",
    "                self.count += 1\n",
    "                print(f\"{self.count} lines processed out of {len(lines)}\")\n",
    "\n",
    "        #Cache the tensor representation of the labels\n",
    "        self.labels_uniq = list(labels_set)\n",
    "        #for idx in range(len(self.labels)):\n",
    "        #    temp_tensor = torch.tensor([self.labels_uniq.index(self.labels[idx])], dtype=torch.long)\n",
    "        #    self.labels_tensors.append(temp_tensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __transform_data__(self, idx, type):\n",
    "        if type == \"data\":\n",
    "            #data = [self.vocab[token] for token in self.data[idx]]\n",
    "            data = vectorizer.fit_transform([self.data[idx]])\n",
    "            data = data.todense()\n",
    "            data = torch.tensor(data).float()\n",
    "            return data\n",
    "        if type == \"label\":\n",
    "            label = vectorizer.fit_transform([self.labels[idx]])\n",
    "            label = label.todense()\n",
    "            label = torch.tensor(label).float()\n",
    "            return label\n",
    "        return None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_item = self.__transform_data__(idx, \"data\")\n",
    "        data_label = self.__transform_data__(idx, \"label\")\n",
    "        #data_tensor = self.data_tensors[idx]\n",
    "        #label_tensor = self.labels_tensors[idx]\n",
    "\n",
    "        return data_label, data_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = '<unk>'\n",
    "vocab.set_default_index(vocab[unk_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PhishDataset(\"processed_data/\", len(vocab), vocab)\n",
    "print(f\"loaded {len(data)} items of data\")\n",
    "print(f\"example = {data.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = torch.utils.data.random_split(data, [.85, .15], generator=torch.Generator(device=device).manual_seed(1))\n",
    "\n",
    "print(f\"train examples = {len(train_set)}, validation examples = {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy\n",
    "# X_train = torch.tensor(X_train).float()\n",
    "# X_test = torch.tensor(X_test).float()\n",
    "# y_train = torch.tensor(y_train.values)\n",
    "# y_test = torch.tensor(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNNNet(len(vocab), TrainingConfig.n_hidden, len(processed_df.label.unique()))\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import numpy as np\n",
    "# train_losses = []\n",
    "# test_losses = []\n",
    "# test_accuracies = []\n",
    "# def train(rnn, X_train, y_train, n_batch_size=TrainingConfig.train_batch_size, n_epoch=TrainingConfig.num_epochs, report_every=50, learning_rate=TrainingConfig.learning_rate, criterion=nn.CrossEntropyLoss()):\n",
    "#     current_loss = 0\n",
    "#     losses = []\n",
    "#     rnn.train()\n",
    "#     optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "#     start = time.time()\n",
    "#     print(f\"training on data set with n = {len(processed_df['label'])}\")\n",
    "\n",
    "#     for iter in range(1, n_epoch + 1):\n",
    "#         rnn.zero_grad()\n",
    "\n",
    "#         output = rnn.forward(X_train)\n",
    "#         loss = criterion(output, y_train)\n",
    "#         train_loss = loss.item()\n",
    "#         train_losses.append(train_loss)\n",
    "#         # optimize parameters\n",
    "#         loss.backward()\n",
    "#         nn.utils.clip_grad_norm_(rnn.parameters(), 3)\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#     # Turn off gradients for validation, saves memory and computations\n",
    "#         with torch.no_grad():\n",
    "#             rnn.eval()\n",
    "#             log_ps = rnn(X_test)\n",
    "#             test_loss = criterion(log_ps, y_test)\n",
    "#             test_losses.append(test_loss)\n",
    "\n",
    "#             ps = torch.exp(log_ps)\n",
    "#             top_p, top_class = ps.topk(1, dim=1)\n",
    "#             equals = top_class == y_test.view(*top_class.shape)\n",
    "#             test_accuracy = torch.mean(equals.float())\n",
    "#             test_accuracies.append(test_accuracy)\n",
    "\n",
    "#         if iter % report_every == 0:\n",
    "#             print(f\"{iter} ({iter / n_epoch:.0%}): \\t average batch loss = {train_losses[-1]}\")\n",
    "#         current_loss = 0\n",
    "\n",
    "#     return all_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def train(rnn, training_data, n_epoch=TrainingConfig.num_epochs, n_batch_size=TrainingConfig.train_batch_size, report_every = 50, learning_rate=TrainingConfig.learning_rate, criterion = nn.CrossEntropyLoss()):\n",
    "    \"\"\"\n",
    "    Learn on a batch of training_data for a specified number of iterations and reporting thresholds\n",
    "    \"\"\"\n",
    "    # Keep track of losses for plotting\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    rnn.train()\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "    start = time.time()\n",
    "    print(f\"training on data set with n = {len(training_data)}\")\n",
    "\n",
    "    for iter in range(1, n_epoch + 1):\n",
    "        rnn.zero_grad() # clear the gradients\n",
    "\n",
    "        # create some minibatches\n",
    "        # we cannot use dataloaders because each of our names is a different length\n",
    "        batches = list(range(len(training_data)))\n",
    "        random.shuffle(batches)\n",
    "        batches = np.array_split(batches, len(batches) //n_batch_size )\n",
    "\n",
    "        for idx, batch in enumerate(batches):\n",
    "            batch_loss = 0\n",
    "            for i in batch: #for each example in this batch\n",
    "                (label, text) = training_data[i]\n",
    "                output = rnn.forward(text)\n",
    "                loss = criterion(output, label)\n",
    "                batch_loss += loss\n",
    "\n",
    "            # optimize parameters\n",
    "            batch_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(rnn.parameters(), 3)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            current_loss += batch_loss.item() / len(batch)\n",
    "\n",
    "        all_losses.append(current_loss / len(batches) )\n",
    "        if iter % report_every == 0:\n",
    "            print(f\"{iter} ({iter / n_epoch:.0%}): \\t average batch loss = {all_losses[-1]}\")\n",
    "        current_loss = 0\n",
    "\n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "all_losses = train(rnn, train_set, report_every=5)\n",
    "end = time.time()\n",
    "print(f\"training took {end-start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
